\section{Challenges Of \spath Caching}\label{sec:competitors}

In this section we qualify the differences between static (no change in cache content at service runtime) and dynamic caching (Cache content is changed at service runtime) of \spathsns. We also introduce our state of the art competitor and explain why it is not an adequate solution for the \spath caching problem.


Using a dynamic cache\footnotemark and calculating the utility of each path is very expensive. If a dynamic cache is used, and we want to ensure it always keeps the most useful paths in the cache, it will be very expensive to calculate the utility of a new query with respect how much it overlaps with existing \spaths (i.e. how many vertices it shares with an existing \spathns) and how likely it will be able to answer a query in the future, thus adding a substantial overhead to query processing. As the utility of a \spath is so expensive to calculate, while the \spath service is running, it violates goal \ref{item:goal2} in section \ref{subsec:goals}.

Using \lru as the cache replacement policy in a dynamic cache ensures that only minimal overhead is added by using a dynamic cache. When a new query is submittet \lru evicts the least recently uesd \spath and keeps the most recently used \spaths in the cache.
\lru, however, has several shortcomings: 
\begin{itemize}
\item It has no way to determine the usefulness of inserting a path (i.e. no scoring function), which violates goal \ref{item:goal3} (Sec. \ref{subsec:goals}). 
\item Because \lru does not have a scoring function then, even if a path $P$ is valuable (covers many potential queries), if a sequence of consecutive queries, which $P$ can not cover, is submitted, then P will be evicted. 
\item \lru also does not have any way to optimize utilization of the cache space available, possibly wasting a lot of space. 
\end{itemize}

If no additional structure is added then querying the cache may require a scan of all paths in the cache to examine the cache can answer a query or not.

\footnotetext{Every time a new query is submitted we consider evicting an old item from the cache and inserting the new query, using a replacement policy e.g. \lru}


If we assume a cache of size 10 (i.e. has space for 10 vertices), using \lruns, which has already had path A \& B inserted (see Fig. \ref{fig:rxmap}), we can anwer 36 out of the 49 possible queries on the map (i.e. all queries not including $v_7$. However, since \lru always does a cache replacement if there are is a cache miss, then a simple query sequence: $P_{4,7},P_{6,7}$ would replace A \& B with \spathns: $\{v_4,v_5,v_7\}$ and $\{v_6,v_5,v_7\}$, reducing the queries the cache can answer to just 12 out of 49 queries. This clearly demonstrate why it is problematic that \lru does not consider the utility of new queries, as well as how the cache ends up wasting space for 4 nodes.





% ---
% more beneficial to fill the cache offline and use a static cache, i.e. not evicting or inserting \spaths while the system is running.
% ----

% Intro to section; Why do we have baseline competitors, which ones will be introduced, and why those? 

% \lru is a competitor, but can not achieve good performance because:
% \begin{itemize}	
% 	\item Has no way to determine the usefulness of adding a path (i.e. no scoring function)
% 	\item Even if a path P is good (covers many queries), then if a sequence of consecutive queries comes which P can not cover, then it will be evicted.
% 	\item Has no way to optimize the number of paths in the cache, so available cache space may go unused.
% 	\item 
% \end{itemize}






\section{Contribution} \label{sec:contribution}

Intro to section \\
Show benefits of a static cache over a dynamic cache.\\
List all advantages of a static cache solution \\
(\textit{Static cache} solves goal 2. has zero maintenance cost after filling the cache.)

explain what will be introduced, which sub-problems are considered, and which subsections presents what.


\begin{table}
\begin{tabular*}{\columnwidth}{|l||p{0.61\columnwidth}|}
\hline
\bf Symbol		& \bf Meaning \\\hline
$SP_{S,T}$		& \spath from S to T \\\hline
$C_{SP}$		& \cet spent on \spath calculation \\\hline
$C_{OH}$		& \cet spent on overhead and maintenence \\\hline
$F_{Time}(C_{SP},C_{OH})$& Total \cet spent on answering a query workload \\\hline
$Cache[i]$		& i'th item in the cache \\\hline
$Utility(P_{S,T})$	& Calculates the benifit of a (sub-)path $P_{S,T}$ \\\hline
$Utility_{_T}(\spath)$	& Finds the total sum of the utility of the \spath and its sub-paths \\\hline
$P_{val}(Cache)$	& Finds the total utility of the content in the cache $\left(\sum\limits_{i=cache[1]}^{cache[size]} Utility_{_T}\left(cache[i]\right)\right)$ \\\hline

\end{tabular*}
\caption{Table of Symbols}
\label{tab:symbols}
\end{table}


\begin{figure}[bht]
  \center
        \includegraphics[width=0.5\textwidth]{figures/fillcache}
        \caption{Insertion of cache elements in offline phase.}
  \label{fig:fillcache}
\end{figure}


\subsection{Benefit model}

We will define our goals more formally, introducing the benefit equations we aim to minimize.



% define benefit equations\\
% Define optimization goal as a mathematical expression to be minimized.

\subsection{Hardness Analysis}
Theoretical analysis showing how hard the problem is to solve for \spath caching.\\
show it is NP-Hard
 

\subsection{Greedy algorithm}
shows in more detail how we propose to solve our problem


\subsection{Statistics extraction}

\subsubsection{Partition map} 
solves goal 1. reduce time on \spath calc as it aids the cache with information on which paths may be useful.\\ 
discussion of kD-Tree.


\subsection{Cache representations and cache concepts} 

\subsubsection{Simple array of paths} - baseline for goal 2, unoptimized and expensive to use.

\subsubsection{Simple array of paths inverted list} - solves goal 2, reduces the query time of the cache.

\subsubsection{Graph representation} - solves goal 1, allows for more paths in cache, which should translate more cache hits.

\subsubsection{Sharing subpaths} - solves goal 1, allows for more paths in cache, translating to more cachehits. Unfortunately has a negative impact on goal 2 as it introduces some overhead in query time.

