\section{Introduction} \label{sec:intro}

This project is motivated by the Microsoft Kinect hardware's ability to capture both image and depth information. 
The images can be captured from the Kinect as a set of raw data dump files or compressed JPEG files. 
This project will focus on how these two sets of output files can be compressed with both lossy and lossless compression techniques on RGB + D image information. D is depth information the a RGB image.

\subsection{Microsoft Kinect Background}

Microsoft Kinect provides to developers with great opportunities for innovative programs, mainly in the areas of gaming and applications for facial recognition.
In the past, it has been described as a "controller-free gaming and entertainment experience" and it is commonly sold bundled with the Microsoft Xbox 360. But just considering it an alternative way to play games is to underestimate its significance. It is a general purpose and low cost 3D input device, not to mention it amazing audio input 
capabilities, enabling it to do speech recognition from several meters away.

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/kinectimage1}
        \caption{Microsoft Kinect}
  \label{fig:kinect}
\end{figure}

Microsoft released a development kit for Kinect June 16, 2011.
It allows developers to build applications with .NET, providing API's for languages like C++, C\# or Visual Basic, using Microsoft Visual Studio 2010 or later. 
A very important feature of Kinect is that developers can use it for their own applications, without paying a license fee.
Essentially the Kinect is a box with 2 cameras, 4 microphones, and infra-red (IR) laser illumination, to obtain color images, sound, and depth data. 
The IR is used as a distance ranging device by having an IR camera measure displacement of a number of generated IR dots. 
The system can measure distance with 1 cm accuracy at 2 meter and has a resolution of 3 mm at 2 meter. 
The depth image is also 640 x 480, i.e. standard VGA resolutions. 
The color image is 1600 x 1200. 
A custom chip processes the data to provide a depth field that is correlated with the color image. 
That is the software can match each pixel with its approximate depth. The preprocessed data is fed to the machine via a USB interface in the form of a depth field map and a color image.

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/Compressionkinectimage2}
        \caption{A Kinect Sensors}
  \label{fig:kinectsensors}
\end{figure}

\subsubsection{Depth map}
The Kinect uses \textit{structured light} to detect distances. The idea is simple, if a known pattern of light is project onto a surface, then the projected spots of light are shifted according to the distance they are reflected back from (see fig. \ref{fig:kinectmeasuredist}).

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/Compressionkinectimage3}
        \caption{Kinect distance measurement}
  \label{fig:kinectmeasuredist}
\end{figure}

By projecting a fixed grid of dots onto a scene and measuring how much each one has shifted when viewed with a video camera you can work how far away each dot was reflected back from. 
The actual implementation is more complicated than this because the IR laser in the Kinect uses a hologram to project a random speckle pattern onto the scene. 
It then measures the offset of each of the points to generate an 11 bit depth map. 
A custom chip does the computation involved in converting the dot map into a depth map so that it can process the data at the standard frame rate.

\subsubsection{Kinect Image}
Kinect has two cameras, one for capturing a color image and the other for capturing 
an IR image. Real-time depth information is provided by the IR camera by calculating a depth map based on its measurements. 
The depth map tells how far the IR camera's pixels are. 
The user doesn't know the depth information of the color image because the two cameras have different characteristics.

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/Compressionkinectimage4}
        \caption{Kinect IR and color camera image.}
  \label{fig:kinectimages}
\end{figure}

The Kinect generates a lot of data. 
One second of video at a resolution of 640 x 480 pixels for both depth and color and at 30 frames per second generates approximately 70Mb of data (640 x 480 * 4 bytes per pixel 
* 30 frames per second * 2 = 73,728,000 bytes), together with a comparatively small amount of audio and skeletal tracking data.
The Kinect uses standard compression techniques to reduce space and bandwidth requirements for all the data it generates. The Kinect uses lossy JPEG for color images and lossless PNG for depth maps, reducing the overall size of a typical recording.

\subsubsection{Kinect Natural User Interface for Windows}
The Natural User Interface (NUI) (see fig. \ref{fig:nuiapi}) is the core of the Kinect for Windows API. It can access the following sensor data in your application:

\begin{itemize}
\item Audio data streamed out by the audio stream.
\item Color image data and depth image data streamed out by the color and depth streams.
\end{itemize}

In addition to the hardware capabilities, the Kinect software runtime implements:
\begin{itemize}
\item A software pipeline that can recognize and track a human body. The runtime converts depth information into skeleton joints, to track the major joints of the human body. This makes it possible to track up to two people in front of the camera.
\item Integration with the Microsoft Speech API's, making developers able to implement a speech recognition engine in their Kinect-enabled application. It is possible to add e.g. voice commands, such as "Start Tracking" or "Stop Tracking", to an application.
\item A tight integration with the Face Tracking SDK, which makes it possible to track human faces.
\end{itemize}

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.48\textwidth]{figures/projectimage1}
        \caption{Natural User Interface API for Kinect}
  \label{fig:nuiapi}
\end{figure}

\subsubsection{Color Image Data on NUI}
Color image data is available at different resolutions and formats. The format determines whether the color image data stream is encoded as RGB, YUV, or Bayer. Only one resolution and format may be used at a time.

The sensor uses a USB 2.0 connection that provides a fixed amount of bandwidth for passing data (480 Mbit). The choice of resolution allows tuning of how the bandwidth is used. High-resolution images send more data per frame and update less frequently, while lower-resolution images update more frequently. Compression can be used, independently of the resolution, to increase frame rate.

Color formats are computed from the same camera data, so all data types represent the same image.
Available formats are given in table \ref{tab:imageformats}. 

\begin{table*}
\center
\begin{tabular}{|l|p{30em}|} \hline
Color Format 	& Description \\\hline
RGB 		& 32-bit, linear X8R8G8B8-formatted color bitmaps, in sRGB color space \\\hline
YUV 		& 16-bit, gamma-corrected linear UYVY-formatted color bitmaps, where the gamma correction in YUV space is equivalent to sRGB gamma in RGB space. Because the YUV stream uses 16 bits per pixel, this format uses less memory to hold bitmap data and allocates less buffer memory. YUV data is available only at the 640x480 resolution and only at 15 fps. \\\hline
Bayer		& 32-bit, linear X8R8G8B8-formatted color bitmaps, in sRGB color space. \\\hline
\end{tabular}
\caption{Kinect NUI supported image formats}
\label{tab:imageformats}
\end{table*}

The Bayer format\footnote{http://en.wikipedia.org/wiki/Bayer\_filter} more closely match the physiology of the human eye by including more green pixels values than blue or red. The Bayer color image data that the sensor returns at 1280x960 is compressed and converted to RGB before transmission to the runtime. The runtime then decompresses the data before it passes the data to any application using the Kinect. The use of compression makes it possible to return color data at frame rates up to 30 fps, but the algorithm leads to some loss of image fidelity.

\subsubsection{Depth Data on NUI}
The depth data stream provides frames in which each pixel contains the Cartesian distance (in millimeters) from the camera plane to the nearest object at that particular x and y coordinate in the depth sensor's field of view. There are two possible ranges for depth data: the default range and the near range which may be chosen by the developer, or automatically by the Kinect.

Applications can process data from a depth stream to support various custom features, such as tracking users' motions and identifying background objects to ignore during play.

Each pixel in the depth stream uses 13 bits for depth data and 3 bits to identify a player. A depth data value of 0 indicates that no depth data is available at that position because all the objects were either too close to the camera or too far away from it. When skeleton tracking is disabled, the 3 bits that identify a player are set to 0.

\subsection{Code Implementation}
{\color{red} is this section really that useful? it does not have much to do with what we do, and I think it may be better just simply omit any direct code references and just describe what a developer needs to do, to use a Kinect}
\subsubsection{Natural User Interface (NUI) App initiation}
To receive the information of Kinect API Receiver, it should through the Runtime object. Thus, the first step for the code implementation should be create the Runtime object and use the method "Initialize" for initiation, also call the method "Uninitialize" for closing the application to close the Kinect.

\begin{verbatim}
Create Runtime Object:
Runtime nui = new Runtime();

Initialize the Runtime Object:
Nui.Initialize();

Close the Runtime Object:
Nui.Uninitialize();
\end{verbatim}

\subsubsection{Color Image Data}
\textit{Image Quantity:} 
The resolution of color image from the sensor of Kinect is 1280 x 1024 pixels. After converting it to RGB format and possibly image compression, it will be passed to the NUI. NUI will decompress the image data and pass to the application using the Kinect. Because image compression is added, the frame rate can be up to to 30 frames per second. It will however decrease if the quality of the image is set too high.

\textit{Image Format:}
RGB (32 bits) and YUV (16 bits)

\textit{Image Acquirement:}
\begin{itemize}
\item Polling Model \\
Using program to call the method "ImageStream.GetNextFrame()", and it will return the ImageFrame Object. The attribute "Image" of ImageFrame Object can get the image data.

\item Event Model \\
Using event method to getting the image data. (e.ImageFrame.Image)
  \begin{itemize}
  \item Runtime.DepthFrameReady
  \item Runtime.VideoFrameReady
  \end{itemize}
\end{itemize}


An example of an image captured from the RGB camera on a Kinect Device is shown on the left in figure \ref{fig:kinectcaptureimage}.

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.48\textwidth]{figures/Compressionkinectimage6}
        \caption{Screen shot of Kinect NUI API output}
  \label{fig:kinectcaptureimage}
\end{figure}

\subsubsection{Capture Depth Image}
The depth image is the image from the IR CMOS camera on the Kinect. An example of how the output looks is shown on the right in figure \ref{fig:kinectcaptureimage}.

Each pixel have 16 bits of depth data for each frame, with a distance range of 850mm - 4000mm. "0" indicates that no depth data is present, or the distance is unknown, e.g. when the Kinect faces a mirror. An Application can implement object tracking using the depth image data.

Supported resolutions of depth stream:
\begin{itemize}
\item 640 x 480
\item 320 x 240
\item 80 x 60
\end{itemize}

\subsubsection{Image Acquirement}
ImageFrame.Image.Bits: Byte array from the top and left, each element of this array has 2
bytes from the ImageFormat object.
ImageFormat.Depth: Starting from the second byte and then count 8 bits. (0,0) distance =
(int)(Bits[0] | Bit[1] $<<$ 8)
ImageFormat.DepthAndPlayerIndex: Kinect system can be detects two human objects throw
the sensor with the Kinect for Windows SDK. Each human object has a user ID (see fig. \ref{fig:humobject}).
\begin{itemize}
\item (0,0) distance = (int)(Bits[0] $>>$ 3 | Bits[1] $<<$ 5)
\item Last three bits of data is the user ID.
\end{itemize}

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.48\textwidth]{figures/projectimage2}
        \caption{Kinect ImageFormat object format}
  \label{fig:humobject}
\end{figure}
