\section{Methodology}

We will in the following introduce the Discrete Cosine Transform and the Wavelet Transform, along with an example of the Haar wavelet. 

After this will will go through the main steps to do lossy JPEG and lossless PNG compression.


\subsection{Discrete Cosine Transform (DCT)}

The JPEG format uses DCT \cite{dct} when as part of its compression, on both RGB and grey level images. 
First, the image is broken into blocks of 8 x 8 pixels, which traversed from left to right, top to bottom. DCT is applied to each block. 
Since DCT is designed to work on pixel values ranging from -128 to 127, the original block
is leveled off by subtracting 128 from each entry. 
Each block is compressed through quantization. The array of compressed blocks that constitute the image is stored in a drastically reduced amount of space. 

Generally, there are less high frequency values than low frequency in each block, and since the human eyes are not sensitive to high frequencies, they can be quantized using fewer bits for storage or communication.
The compressed images are accepted as the same by the human eye. 
After quantization of the DCT coefficients of each 8 x 8 block, compression will be applied to the coefficients.
The final step for compression is to quantize the matrix before storage which converted by an encoder to a stream of binary data. 
After quantization, most of the coefficients will be equal to zero because the encoding  of quantized coefficient is done in as zig-zag sequence in JPEG image. 
For decompression of image, reconstruction is done by decoding the bit stream representing the quantized matrix. The process uses the Inverse Discrete Cosine Transform (IDCT).

\subsection{Wavelet Transform (WT)}

Wavelet transform \cite{wt} is a computational tool for signal and image processing, which is useful for compressing digital image files to reduce storage requirements or transmitting more reliably over low bandwidth networks. 
Wavelet analysis is based on a short duration wavelet of a specific center frequency. 
It provides time frequency representation simultaneously which also provides variable resolution at high frequency wavelet transform gives good time resolution and poor frequency resolution whereas at low frequency wavelet transform gives good frequency resolution and poor time resolution.
Computing the wavelet transform by recursively averaging and differentiating coefficients, is
called a filter bank which generalize to other types of wavelet where no information has
been gained or lost by the process. 
Reconstruction of the image to any resolution (up to the original resolution) can be done recursively by adding and subtracting the coefficient from the lower resolution version.
The advantage of using wavelet transform is that most of large numbers coefficient
turn out to be very small in magnitude. Wavelet transform only has small error in reconstructed image, giving a form of lossy image compression by truncating or removing small coefficients from
the representation.


\subsection{2D Haar Wavelet Image Compression}
In this project, we will use the Haar wavelets in two dimensions for image compression.

The procedure for the 2D Haar wavelet compression is shown as follow:
\begin{enumerate}
\item The first step is to apply the 1D transform to each row of the original image;
\item  The second step is to apply the 1D transform to each column of the 
row-transformed image;
\item Such a procedure can be iteratively implemented for multi-scale 2D Haar wavelet 
compression.
\end{enumerate}

For 2D Haar Wavelet Transform, the original image will apply the 2D Haar Wavelet and divide the original image into 4 areas: top left, top right, bottom left, right, top and bottom. First, the original image figure \ref{fig:2dhaarwavelet}A performs haar wavelet with 2 times from left to right and then from top to bottom to produce the image figure \ref{fig:2dhaarwavelet}B. 
Second, it will repeat perform the f from Figure \ref{fig:2dhaarwavelet}B to Figure \ref{fig:2dhaarwavelet}C.

\begin{figure*}[!Hbt]
  \center
  \begin{tabular}{ccc}
    \includegraphics[width=0.35\textwidth]{figures/barbara}&
    \includegraphics[width=0.35\textwidth]{figures/"Haar_2D_1"}&
    \includegraphics[width=0.35\textwidth]{figures/"Haar_2D_2"}\\
    (A)&(B)&(C)\\
  \end{tabular}
    \caption{2D Haar wavelet image transform}
  \label{fig:2dhaarwavelet}
\end{figure*}

\subsection{Lossy JPEG}
JPEG \cite{jpeg} is a compression technique for digital images, the compression ratio can be adjusted. Usually, it achieves 10:1 compression ratio with insignificant perceptible loss in image quality.

\subsubsection{JPEG Compression}
JPEG lossy compression (see fig. \ref{fig:jpegcompressflow}) based on the DCT (discrete cosine transform). It converts the spatial domain to the frequency domain. The human visual system can not perceive high-frequency information well, such as color hue, so there's quantization procedure to reduce the high frequency information. Quantization is a method to reduce the number of unique values by compressing them into ranges, and then use the small entropy to encode the numbers. The JPEG compression is usually lossy, but there's an option for lossless compression, it's defined under JPEG 2000 standard.


\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/jpegCompressionimage1}
        \caption{The flow of JPEG compression}
  \label{fig:jpegcompressflow}
\end{figure}

\subsubsection{Color space transformation}
The image needs to be converted from RGB to another color space, such as HSI or YCbCr space, Y component represent the brightness of a pixel, and the Cb and Cr represent the chrominance. 

\subsubsection{Down-sampling and Block Splitting}
The human eye is more sensitive to the brightness of an image ( the Y component or intensity channel) than the Cb and Cr component which are the hue and color saturation. So we can reduce the spatial resolution of the Cb and Cr components, which we can call it down-sampling. After down sampling, we can split each channel into 8x8 blocks. 

\subsubsection{Discrete Cosine Transform}
Each 8x8 block of each component YCbCr will be converted to the frequency domain, by using DCT (discrete cosine transform). After transformation, the low frequency pattern will be on left top side and high frequency on the right bottom side.

\subsubsection{Quantization}
The human eye is good at seeing the small differences in brightness, but not good to see the high frequency brightness. So we can reduce the high frequency components in order to save the space, we divide each component by a constant, and then rounding it to the nearest integer, as a result, many high frequency component will be rounded to 0, and many of the rest become small integer or negative value, which take less bits to represent.


\subsubsection{Huffman Coding}

Huffman coding \cite{huffman} is an entropy encoding algorithm used for lossless data compression. It use a variable-length code table for encoding the source data, the method is base on the estimated probability of each value occurrence. Huffman coding uses \textit{prefix tree codes} to choose the representation for each value, it uses shorter bit strings to encode more likely values. 

Figure \ref{fig:huffmancoding} illustrates how the string "Kenn" could be encoded with Huffman:


\vspace{0.7em}\begin{tabular}{ll}
0  & represent "n"\\
10 & represent "e"\\
11 & represent "k"\\
\end{tabular}

\begin{figure}[hbt]
  \center
        \includegraphics[width=0.45\textwidth]{figures/huffmancoding}
        \caption{The flow of JPEG compression}
  \label{fig:huffmancoding}
\end{figure}

\subsection{Lossless PNG}

Portable Network Graphics (PNG) \cite{png} is a raster graphics file format that supports lossless data compression. PNG was patent-free to replace Graphics Interchange Format (GIF). PNG has better compression an interlacing than GIF and adds new features compared to GIF, e.g. gamma storage, alpha channel, true color support, and error detection. PNG supports two methods of transparency: one-color masking and alpha channel. PNG's full alpha channel makes it possible to create beautiful glows and drop which layer over different-colored backgrounds perfectly. PNG was designed for distribution of images on the internet not for professional graphics.

Lossless PNG uses two stages for compression process: Pre-compression: Filtering (Prediction) and Compression: DEFLATE.

\subsubsection{Filtering (Prediction)}
Before using DEFLATE for compression, the data should be pre-compress first. It is a single filter method that used for the entire image, while for each image line, a filter type is chosen that transforms the data for easier compression. For this method, the filter predicts the value of each pixel based on the values of previous neighboring pixels, and subtracts the predicted color of the pixel from the actual value. There are five filter types for this filter method; each type predicts the value of each byte based on the corresponding byte of the pixel to the left (A), above (B), above and to the left (C) or some combination thereof, and encodes the difference between the predicted value and the actual value. Filters are applied to byte values, not pixels; pixel values may be one or two bytes, or several values per byte, but never cross byte boundaries. The filter types are given in table \ref{tab:filtertypes}

\begin{table}
\begin{tabular}{|l|l|p{10em}|}\hline
Byte		& Filter	& Predicted value \\\hline
0		& None	 	& Zero (so that the raw byte value passes through unaltered) \\\hline
1		& Sub		& Byte A (to the left) \\\hline
2		& Up		& Byte B (above) \\\hline
3		& Average	& Mean of bytes A and B, rounded down \\\hline
4		& Paeth		& A, B, or C, whichever is closest to $p = A + B - C$ \\\hline
\end{tabular}
\caption{PNG Filter types}
\label{tab:filtertypes}
\end{table}

\subsubsection{Compression: DEFLATE}
PNG using called "DEFLATE" compression scheme (the same engine used to "zip" files with zlib, gzip, etc.).  "DEFLATE" is a lossless data compression algorithm, used a combination of the LZ77 algorithm and Huffman coding. Like GIF, Compression of PNG works on rows of pixels, taking advantage of repetition in bytes of information. By use of internal filters, it can take advantage of some vertical patterns as well. PNG's compression engine typically compresses images 10-30\% better than GIF (and up to 39\% better under optimal conditions). During the compression stage, the encoder of DEFLATE will chooses the amount of time spent looking for matching strings. The zlib/gzip reference implementation allows the user to select from a sliding scale of likely resulting compression-level vs. speed of encoding. Options range from -0 (do not attempt compression, just store uncompressed) to -9 representing the maximum capability of the reference implementation in zlib/gzip.
 
Other Deflate encoders have been produced, all of which will also produce a compatible bitstream capable of being decompressed by any existing Deflate decoder. 
Typical Compression Ratios for JPEG and PNG

\begin{table*}
\begin{tabular}{|l||l|p{18em}|}\hline
Format		& Typical Compression Ratios 	& Description \\\hline
JPEG (High)	& 10:1 - 20:1 			& High quality - has little or no  loss in image quality with continuous tone originals. Worse results for flat color and sharp-edge art.\\\hline
JPEG (Medium)	& 30:1 - 50:1			& Moderate quality - usually the best choice for the Web.\\\hline
JPEG (Low) 	& 60:1 - 100:1			& Poor quality - suitable for thumbnails and previews. Visible blockiness.\\\hline
PNG 		& 10-30\% smaller than GIFs	& PNG's behave similarly to GIF's, only better; they work best with flat-color, sharp-edged art. PNGs compress both horizontally and vertically, so solid blocks of color generally compress best \\\hline
\end{tabular}
\end{table*}

